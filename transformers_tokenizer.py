import pandas as pd
import numpy as np

# pip install keras-bert
# pip install tensorflow

from keras_bert import Tokenizer

token_dict = {
    '[CLS]': 0,
    '[SEP]': 1,
    'un': 2,
    '##aff': 3,
    '##able': 4,
    '[UNK]': 5,
}

tokenizer = Tokenizer(token_dict)
print(tokenizer.tokenize('unaffable'))

indices, segments = tokenizer.encode('unaffable')

print(indices)  # Should be `[0, 2, 3, 4, 1]`
print(segments)  # Should be `[0, 0, 0, 0, 0]`

print(tokenizer.tokenize(first='unaffable'))
# The result should be `['[CLS]', 'un', '##aff', '##able', '[SEP]', 'é’¢', '[SEP]']`
indices, segments = tokenizer.encode(first='unaffable', max_len=10)
print(indices)  # Should be `[0, 2, 3, 4, 1, 5, 1, 0, 0, 0]`
print(segments)  # Should be `[0, 0, 0, 0, 0, 1, 1, 0, 0, 0]`